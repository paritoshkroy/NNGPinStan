---
title: "Nearest Neighbor GP Models in Stan"
output:
  github_document:
    toc: true
    toc_depth: 2
    fig_width: 5
    fig_height: 5
    math_method:
        engine: webtex
        url: https://latex.codecogs.com/svg.image?
bibliography: bibliography.bib
biblio-style: apalike
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Gaussian Process

### Inference Procedure
Let $\mathbf{y} = (y(\mathbf{s}_1),\ldots, y(\mathbf{s}_n))^\prime$ be the $n$--dimensional vector of data over $n$ locations $\mathbf{s}_1,\ldots,\mathbf{s}_n$ in $d$--dimensional domain $\mathcal{D} \in \mathbb{R}^{d}$. Using marginalization with respect to $\mathbf{z}^\prime = (z(\mathbf{s}_1),\ldots,z(s_n))$, it can be shown that $\mathbf{y}$ is distributed as multivariate normal with $\mathbb{E}[\mathbf{y}] = \mathbf{X}\boldsymbol{\theta}$ and $\text{Var}[\mathbf{y}] = \sigma^2 \mathbf{B} + \tau^2\mathbf{I}$, where $\mathbf{X}$ is a $n \times p$ design matrix based on the vector of covariates $\mathbf{x}(\mathbf{s}_i)$ and $\mathbf{B}$ is the $n$--dimensional correlation matrix whose $(i,j)$th elements is $\mathbf{B}_{ij} = \rho(||\mathbf{s}_i-\mathbf{s}_j||)$. Under the Bayesian paradigm, model specification is complete after assigning a prior distribution for $\boldsymbol{\beta}$, $\sigma$, $\ell$ and $\tau$. Then, following Bayes' theorem, the joint posterior distribution of $\boldsymbol{\Phi} = \{\boldsymbol{\theta}, \sigma, \ell, \tau\}$ is proportional to

\[\pi(\boldsymbol{\Phi} \mid \mathbf{y}) \propto \mathcal{N}\left(\mathbf{y} \mid \mathbf{X}\boldsymbol{\theta}, \mathbf{V}\right) \; \pi(\boldsymbol{\Phi}),\]

where $\mathbf{V} = \sigma^2 \mathbf{B} + \tau^2\mathbf{I}$ and $\pi(\boldsymbol{\Phi})$ denotes the prior distribution assigned to $\boldsymbol{\Phi}$. In practice, the distribution $\pi(\boldsymbol{\Phi} \mid \mathbf{y})$ does not have a closed-form, and Markov chain Monte Carlo (MCMC) sampling methods are commonly employed to approximate this distribution. These methods are straightforward to implement using modern statistical computing platforms such as \texttt{BUGS}, \texttt{JAGS} , \texttt{NIMBLE}, and \texttt{Stan}. MCMC methods provide samples from the posterior distribution, which can be used to estimate various summary statistics. Once samples from the posterior distribution are available, predictions to unobserved locations follow straightforwardly.

### Spatial Prediction
To predict the responses $\mathbf{y}^{\star} = (y(\mathbf{s}_1^\star),\ldots,y(\mathbf{s}_{n^\star}^\star))^\prime$ at any set of $n^\star$ unobserved locations $\mathbf{s}_1^\star,\ldots,\mathbf{s}_{n^\star}^\star$, consider the joint vector $(\mathbf{y}^{\star\prime},\mathbf{y}^\prime)$, under a Gaussian process assumption whose distribution is $(n^\star + n)$--dimensional multivariate normal. Consequently, the conditional distribution $\mathbf{y}^\star$ given $\mathbf{y}$ is $n^\star$--dimensional multivariate normal with conditional mean and variance, respectively, given by 

$$
\begin{aligned}
\mathbb{E}[\mathbf{y}^\star \mid \mathbf{y}] = 
\mathbf{X}^\star\boldsymbol{\theta} + \mathbf{V}^{\text{pred-to-obs}} \mathbf{V}^{-1} (\mathbf{y} - \mathbf{X}\boldsymbol{\theta})\\
\text{and} \text{Var}[\mathbf{y}^\star \mid \mathbf{y}] = \mathbf{V}^\star - \mathbf{V}^{\text{pred-to-obs}} \mathbf{V}^{-1} \mathbf{V}^{\text{obs-to-pred}},
\end{aligned}
$$

which is used to perform the prediction, where $\mathbf{X}^\star$ is the $n^\star \times p$ design matrix of covariates at prediction locations. The covariance matrix $\mathbf{V}^\star$ is equal to $\sigma^2 \mathbf{B}^\star + \tau^2 \mathbf{I}$, where $\mathbf{B}^\star$ denotes the $n^\star$--dimensional spatial correlation matrix among the prediction locations. The component $\mathbf{V}^{\text{pred-to-obs}}$ is equal to $\sigma^2 \mathbf{B}^{\text{pred-to-obs}}$, where $\mathbf{B}^\text{pred-to-obs}$ denotes the $n^\star \times n$ spatial correlation matrix between prediction and observed locations.

Note that the model above specification is referred to as the marginal or response Gaussian model, and the inference and prediction procedures are outlined based on it. However, this model can be represented hierarchically as follows:
$$\mathbf{y} \mid \boldsymbol{\theta}, \mathbf{z} \sim \mathcal{N} \left(\mathbf{X}\boldsymbol{\theta} + \mathbf{z}, \tau^2\mathbf{I}\right),$$

$$\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{B}).$$

In practice, the response Gaussian process model is often preferred for efficient parameter estimation, as it circumvents the need to estimate the latent vector $\mathbf{z}$ directly. Instead, in a Bayesian analysis, once posterior samples for the parameters are obtained, estimates for $\mathbf{z}$ can be recovered through composition sampling techniques.

### Recovery of the Latent Component
One might be interested in the posterior distribution of the latent spatial component $z(\boldsymbol{s})$. The inference using joint posterior distribution in equation~\eqref{chap2:gp_posterior_dist} ignores the estimation of the latent vector $\mathbf{z}^\prime = (z(\boldsymbol{s}_1), \ldots, z(\boldsymbol{s}_n))$ during model fitting. Nevertheless, we can recover the distribution of vector $\mathbf{z}$ components via composition sampling once samples from the posterior distribution of the parameters are available. Note that the joint posterior distribution of $\mathbf{z}$ is
$$
\begin{align*}
\pi(\mathbf{z} \mid \mathbf{y}) &= \int \pi(\boldsymbol{\Phi}, \mathbf{z} \mid \mathbf{y}) \; \mathrm{d} \boldsymbol{\Phi}\\
&= \int \pi(\mathbf{z} \mid \boldsymbol{\Phi}, \mathbf{y}) \; \pi(\boldsymbol{\Phi} \mid \mathbf{y}) \; \mathrm{d} \boldsymbol{\Phi},
\end{align*}
$$
and
$$
\begin{align}
\pi(\mathbf{z} \mid \boldsymbol{\Phi}, \mathbf{y})
&\propto \mathcal{N}(\mathbf{z} \mid \mathbf{0}, \sigma^2 \mathbf{B}) \; \mathcal{N}\left(\mathbf{y} \mid \mathbf{X}\boldsymbol{\theta} + \mathbf{z}, \tau^2\mathbf{I}\right)\\
&\propto \exp\left\{-\frac{1}{2\sigma^2} \mathbf{z}^\prime \mathbf{B}^{-1} \mathbf{z}\right\} \; \exp\left\{-\frac{1}{2\tau^2} (\mathbf{y} - \mathbf{X}\boldsymbol{\theta} -\mathbf{z})^\prime (\mathbf{y} - \mathbf{X}\boldsymbol{\theta} - \mathbf{z})\right\} \nonumber\\
&\propto \exp\left\{-\frac{1}{2}\mathbf{z}^\prime \left(\frac{1}{\tau^2} \mathbf{I} + \frac{1}{\sigma^2}\mathbf{B}^{-1} \right) \mathbf{z} - \mathbf{z}^\prime \left(\frac{1}{\tau^2} \mathbf{I}\right) (\mathbf{y} - \mathbf{X}\boldsymbol{\theta})\right\},
\end{align}
$$
which is the kernel of the multivariate normal distribution with mean 
and covariance, 
$$
\begin{align}
\mathbb{E}[\mathbf{z} \mid \mathbf{y}] &= \left(\dfrac{1}{\tau^2} \mathbf{I} + \dfrac{1}{\sigma^2}\mathbf{B}^{-1} \right)^{-1} \left(\dfrac{1}{\tau^2} \mathbf{I}\right) (\mathbf{y} - \mathbf{X}\boldsymbol{\theta}),\\
\text{and}\; \text{Var}[\mathbf{z} \mid \mathbf{y}] &= \left(\dfrac{1}{\tau^2} \mathbf{I} + \dfrac{1}{\sigma^2}\mathbf{B}^{-1} \right)^{-1},
\end{align}
$$
respectively. Therefore, posterior samples for $\mathbf{z}$ can be obtained by drawing samples from  $\pi(\mathbf{z} \mid \boldsymbol{\Phi}, \mathbf{y})$ one-for-one for each posterior sample of $\boldsymbol{\Phi}$. These are post-MCMC calculations; hence, sampling is not very expensive. Given the posterior samples for $\mathbf{z}$ associated with observed locations and $\boldsymbol{\Phi}$, it is also possible to obtain samples of the distribution of $n^\star$--dimensional vector $\mathbf{z}^\star$ of the values of $z(\mathbf{s})$ at unobserved locations $\mathbf{s}_{1}^\star, \ldots, \mathbf{s}_{n^\star}^\star$ via composition sampling. The procedure involves assuming joint vectors $(\mathbf{z}^{\star\prime},\mathbf{z}^\prime)$ which follows $(n^\star + n)$--dimensional multivariate normal distribution and conditional distribution of $\mathbf{z}^\star$ given $\mathbf{z}$ is used to draw samples for $\mathbf{z}^\star$. The conditional distribution is $n^\star$--dimensional multivariate normal with mean $\mathbf{E}[\mathbf{z}^\star \mid \mathbf{z}] = \mathbf{B}^{\text{pred-to-obs}} \mathbf{B}^{-1} \mathbf{z}$ and variance $\text{Var}[\mathbf{z}^\star \mid \mathbf{z}] = \sigma^2 (\mathbf{B}^\star - \mathbf{B}^{\text{pred-to-obs}} \mathbf{B}^{-1} \mathbf{B}^{\text{obs-to-pred}})$.


### Stan implementation of marginal model
```{r results='markup',echo=FALSE,comment=""}
cat(paste(readLines("StanFiles/MarginalLinGeostatModel.stan"), collapse = "\n"))
```

### Computational Complexity of the Inference of GP Model

## NNGP approximation of a GP
@datta2016hierarchical developed the NNGP as a sparse approximation of to a full GP. It generalizes the idea of @vecchia1988estimation from nearest neighbor approximation of a data likelihood to the nearest neighbor approximation of the likelihood of realizations of the process $z(\boldsymbol{s})$, where the nearest neighbors set of $\boldsymbol{s}$ is defined based upon the Euclidean distance [@datta2021sparse]. Both the nearest neighbor approximations builds upon the idea that the joint distribution for a random vector $\boldsymbol{z}$ can be looked upon as a directed acyclic graph (DAG) to construct sparse models for $\boldsymbol{z}$ by limiting the size of the set of parents of each node [@banerjee2017high; @finley2019efficient]. For a GP $z(\boldsymbol{s})$ with mean zero and covariance function $C$, let $z(\boldsymbol{s}_1),\ldots,z(\boldsymbol{s}_n)$ denotes the $n$ realizations of $z(\boldsymbol{s})$ at $\boldsymbol{s}_1,\ldots,\boldsymbol{s}_n$ and $\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)$ denotes the set of $m$ nearest neighbors of them. Then the NNGP is defined as the nearest neighbor approximation of the likelihood of $z(\boldsymbol{s}_1),\ldots,z(\boldsymbol{s}_n)$ is given by
$$
\begin{align}
\label{eq_nngp_lik}
f(z(\boldsymbol{s}_1)) \prod_{i=2}^{n} f(z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)})
\end{align}
$$
where $z(\boldsymbol{s}_1) \sim \mathcal{N}(0,C_{\boldsymbol{s}_1,\boldsymbol{s}_1})$ and $\boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}$ is the set of $m$ nearest neighbor of the realizations. The conditional distribution $f(z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)})$ is an univariate normal $\mathcal{N}(z(\boldsymbol{s}_i) | \mu_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}}, \sigma^2_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}})$ and which derived from the following multivariate normal
$$
\begin{align}
\begin{pmatrix}
z(\boldsymbol{s}_i)\\
\boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}\\
\end{pmatrix} \sim \mathcal{N}_{m+1}\left(
\begin{bmatrix}
0\\
0
\end{bmatrix},
\begin{bmatrix}
C_{\boldsymbol{s}_i\,\boldsymbol{s}_i} & \boldsymbol{C}^\top_{\boldsymbol{s}_i,\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}\\
\boldsymbol{C}_{\boldsymbol{s}_i,\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)} & \boldsymbol{C}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i),\mathcal{N}(\boldsymbol{s})({\boldsymbol{s}_i})}
\end{bmatrix}
\right),
\end{align}
$$
where
$$
\begin{align}
\label{eq_nngp_con_moments}
\begin{split}
\mu_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}} &= \boldsymbol{C}^\top_{\boldsymbol{s}_i,\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)} \boldsymbol{C}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i),\mathcal{N}(\boldsymbol{s})({\boldsymbol{s}_i})}^{-1} \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}\\
\sigma^2_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}} &= C_{\boldsymbol{s}_i\,\boldsymbol{s}_i} - \boldsymbol{C}^\top_{\boldsymbol{s}_i,\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)} \boldsymbol{C}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i),\mathcal{N}(\boldsymbol{s})({\boldsymbol{s}_i})}^{-1} \boldsymbol{C}_{\boldsymbol{s}_i,\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}\\
\end{split}
\end{align}
$$
The conditional distribution $f(z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)})$ can also be viewed as the likelihood from the generative model $z(\boldsymbol{s}_i) = \boldsymbol{a}_i \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)} + e(\boldsymbol{s}_i)$, where $\boldsymbol{a}_i = \boldsymbol{C}^\top_{\boldsymbol{s}_i,\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)} \boldsymbol{C}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i),\mathcal{N}(\boldsymbol{s})({\boldsymbol{s}_i})}^{-1}$  and $e(\boldsymbol{s}_i) \sim i.i.d\, \mathcal{N}(0,d_{i})$ where $d_1 = C_{\boldsymbol{s}_i,\boldsymbol{s}_i}$ and $d_i = \sigma^2_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}}\; i=2,\ldots,n$. Therefore, the likelihood in \eqref{eq_nngp_lik} can be written as the following set of linear models, $z(\boldsymbol{s}_1) = 0 + e(\boldsymbol{s}_1)$ and for $i=2,\ldots,n$,
\begin{align}
z(\boldsymbol{s}_i) = a_{i1} z(\boldsymbol{s}_1) + a_{i2} z(\boldsymbol{s}_2) + \cdots + a_{i,i-1} z(\boldsymbol{s}_{i-1}) + e(\boldsymbol{s}_i).
\end{align}
where $a_{ij}$ is the $k$th element of $\boldsymbol{a}_i$ if $\boldsymbol{s}_j$ is the $k$th neighbor of $\boldsymbol{s}_i$ and $a_{ij} =0$ if $\boldsymbol{s}_j$  is not a neighbor of $\boldsymbol{s}_i$. In matrix form, it can be written as $\boldsymbol{z} = \boldsymbol{A} \boldsymbol{z} + \boldsymbol{e}$, where $\boldsymbol{A}$ is $n\times n$ strictly lower-triangular and $\boldsymbol{e} \sim \mathcal{N}\left(\mathbf{0}, \boldsymbol{D}\right)$ and $\boldsymbol{D}$ is a diagonal with elements $d_{i}$ for $i=2,\ldots,n$. Therefore, $\mathbf{I}-\boldsymbol{A}$ is a nonsingular matrix and $\boldsymbol{z} \sim \mathcal{N}\left(\mathbf{0},(\mathbf{I}-\boldsymbol{A})^{-1}\boldsymbol{D}(\mathbf{I}-\boldsymbol{A})^{-\top}\right)$ corresponds to a generative multivariate Gaussian model $\boldsymbol{z} \sim \mathcal{N}\left(\mathbf{0},\widetilde{\boldsymbol{C}}\right)$ for the process realizations, where $\widetilde{\boldsymbol{C}} = (\mathbf{I}-\boldsymbol{A})^{-1}\boldsymbol{D}(\mathbf{I}-\boldsymbol{A})^{-\top}$ [@datta2021sparse].  @datta2016hierarchical shows that this is a valid approximation of the likelihood corresponding to the GP model $z(\boldsymbol{s})$ with zero mean and covariance function $C$. However, specification of a valid GP over the entire domain was completed by defining prediction distribution of $z(\boldsymbol{s})$ at new locations conditional on $\boldsymbol{z}$. For the prediction of the latent process $z(\boldsymbol{s})$ at a set of $n_0$ new locations $\{\boldsymbol{s}_{01},\ldots,\boldsymbol{s}_{0n_0}\} \notin \{\boldsymbol{s}_1,\ldots,\boldsymbol{s}_n\}$,  @datta2016hierarchical specified the conditional distribution of $z(\boldsymbol{s}_{0i}) | \boldsymbol{z}$ independently, which is given by
$$
\begin{aligned}
z(\boldsymbol{s}_{0i}) | \boldsymbol{z} \sim \mathcal{N}\left(\boldsymbol{C}^\top_{\boldsymbol{s}_{0i},\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i})} \boldsymbol{C}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i}),\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i})}^{-1} \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i})}, C_{\boldsymbol{s}_{0i}\,\boldsymbol{s}_{0i}} - \boldsymbol{C}^\top_{\boldsymbol{s}_{0i},\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i})} \boldsymbol{C}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i}),\mathcal{N}(\boldsymbol{s})({\boldsymbol{s}_{0i}})}^{-1} \boldsymbol{C}_{\boldsymbol{s}_{0i},\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i})}\right),
\end{aligned}
$$
where the $\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_{0i})$ is the $m$ nearest neighbors of $\boldsymbol{s}_{0i}$ in $\{\boldsymbol{s}_1,\ldots,\boldsymbol{s}_n\}$ instead of $\{\boldsymbol{s}_{01},\ldots,\boldsymbol{s}_{0n_0}\}$. One of the benefits of the NNGP approximation is computational as the matrix $\boldsymbol{A}$ is sparse. Computing the likelihood in \eqref{eq_nngp_lik} involves only computing the conditional mean and variances in \eqref{eq_nngp_con_moments} and requiring $\mathcal{O}(nm^3)$ flops and $\mathcal{O}(nm^2)$ storage as opposed to $\mathcal{O}(n^3)$ flops and $\mathcal{O}(n^2)$ storage for computing the full GP likelihood. However, Bayesian hierarchical modeling of a large spatial data using the NNGP is still challenging. In particular, when the latent process cannot be marginalized in the models and treated as parameters that must be sampled. MCMC sampler becomes inefficient due to high-dimensional parameter space. To a similar problem @wang2019efficient showed that non-centered parameterization to both location and scale parameters of the latent NNGP improves the MCMC efficiency by implementing it in Stan. This implies that the model $\mathcal{N}\left(z(\boldsymbol{s}_i) | \mu_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}}, \sigma^2_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}}\right)$ in \eqref{eq_nngp_lik} is parameterized as $z(\boldsymbol{s}_i) |  z_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)} = \mu_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}} + \sigma_{z(\boldsymbol{s}_i) \mid \boldsymbol{z}_{\mathcal{N}(\boldsymbol{s})(\boldsymbol{s}_i)}} v(\boldsymbol{s}_i)$ where $v(\boldsymbol{s}_i)$ is independent Gaussian noise with mean zero and variance one.

## References


